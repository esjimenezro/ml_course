{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function and Backpropagation\n",
    "\n",
    "Now, we're going to study an algorithm for fitting the parameters of a Neural Network, known as the Backpropagation algorithm. Before doing that, we will define the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cost Function\n",
    "\n",
    "To define the cost function for Neural Networks, let's consider the following general neural network with $L$ layers for **classification problems**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NN](figures/neural_network_general.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we have $m$ training examples $\\{(x^{(1)}, y^{(1)}), \\dots (x^{(m)}, y^{(m)})\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For binary classification we would have $y\\in\\{0,1\\}$, and $K=1$ output units.\n",
    "\n",
    "- For multi-class classification we have $y\\in\\mathbb{R}^K$ and $K$ output units. For example, for $K=4$:\n",
    "  $$\n",
    "  y = \\left[\\begin{array}{c}\n",
    "  1 \\\\\n",
    "  0 \\\\\n",
    "  0 \\\\\n",
    "  0\n",
    "  \\end{array}\\right], \\left[\\begin{array}{c}\n",
    "  0 \\\\\n",
    "  0 \\\\\n",
    "  1 \\\\\n",
    "  0\n",
    "  \\end{array}\\right]\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens from layer $l$ to layer $l+1$ ($l\\in\\{1,\\dots,L-1\\}$)?\n",
    "\n",
    "![transition](figures/transition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we introduce the intermediate variables $z_j^{(l+1)}$ for $j\\in\\{1, \\dots, s_{l+1}\\}$, defined as\n",
    "\n",
    "$$\n",
    "z_j^{(l+1)} = \\Theta_{j0}^{(l)} + \\Theta_{j0}^{(l)}a_1^{(l)} + \\dots + \\Theta_{js_l}^{(l)} a_{s_l}^{(l)}.\n",
    "$$\n",
    "\n",
    "Then, the $j$-th unit activation is defined as\n",
    "\n",
    "$$\n",
    "a_j^{(l+1)} = g(z_j^{(l+1)}),\n",
    "$$\n",
    "\n",
    "where $g$ is the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently,\n",
    "\n",
    "\\begin{align}\n",
    "z^{(l+1)} & = \\Theta^{(l)} \\left[\\begin{array}{c} 1 \\\\ a^{(l)} \\end{array}\\right] \\\\\n",
    "a^{(l+1)} & = g(z^{(l+1)}),\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\Theta^{(l)} = \\left[\n",
    "\\begin{array}{cccc}\n",
    "\\Theta_{10}^{(l)}       & \\Theta_{11}^{(l)}       & \\dots  & \\Theta_{1s_l}^{(l)}  \\\\\n",
    "\\Theta_{20}^{(l)}       & \\Theta_{21}^{(l)}       & \\dots  & \\Theta_{2s_l}^{(l)}  \\\\\n",
    "\\vdots                  & \\vdots                  & \\ddots & \\vdots               \\\\\n",
    "\\Theta_{s_{l+1}0}^{(l)} & \\Theta_{s_{l+1}1}^{(l)} & \\dots  & \\Theta_{s_{l+1}s_l}^{(l)} \n",
    "\\end{array}\\right],\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the form of the hypothesis function?\n",
    "\n",
    "Following the above analysis, in the output layer $L$:\n",
    "\n",
    "$$\\left\\{\n",
    "\\begin{align}\n",
    "z^{(L)} & = \\Theta^{(L-1)} \\left[\\begin{array}{c} 1 \\\\ a^{(L-1)} \\end{array}\\right] \\in\\mathbb{R}^{K} \\\\\n",
    "a^{(L)} & = h_{\\Theta}(x) = g(z^{(L)})\\in\\mathbb{R}^{K}.\n",
    "\\end{align}\\right.\n",
    "$$\n",
    "\n",
    "In the hidden layers $l = 2,\\dots, L-1$:\n",
    "\n",
    "$$\\left\\{\n",
    "\\begin{align}\n",
    "z^{(l)} & = \\Theta^{(l-1)} \\left[\\begin{array}{c} 1 \\\\ a^{(l-1)} \\end{array}\\right] \\in\\mathbb{R}^{s_l} \\\\\n",
    "a^{(l)} & = g(z^{(l})\\in\\mathbb{R}^{s_l}.\n",
    "\\end{align}\\right.\n",
    "$$\n",
    "\n",
    "In the input layer:\n",
    "\n",
    "$$\n",
    "a^{(1)} = x \\in\\mathbb{R}^{n}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the form of the cost function?\n",
    "\n",
    "The cost function we use for this class of neural networks is a generalization of the cost function for the logistic regression:\n",
    "\n",
    "$$\n",
    "J(\\Theta) = -\\frac{1}{m} \\left[\\sum_{t=1}^{m} \\sum_{k=1}^{K} y^{(t)}_k \\log(h_{\\Theta}(x^{(t)})_k) + (1-y^{(t)}_k) \\log(1 - h_{\\Theta}(x^{(t)})_k)\\right] + \\frac{\\lambda}{2m} \\sum_{l=1}^{L-1} \\sum_{i=1}^{s_{l+1}} \\sum_{j=1}^{s_l} \\left(\\Theta_{ij}^{l}\\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, in order to use a gradient based optimization method, we need to compute\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta),\n",
    "$$\n",
    "\n",
    "for each $i\\in\\{1, \\dots, s_{l+1}\\}$, $j\\in\\{0, \\dots, s_{l}\\}$, and $l\\in\\{1, \\dots, L-1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the *cost for the example $t$ at the output $k$*, $c_k^{(t)}$ as:\n",
    "\n",
    "$$\n",
    "c_k^{(t)} = -(y^{(t)}_k \\log(h_{\\Theta}(x^{(t)})_k) + (1-y^{(t)}_k) \\log(1 - h_{\\Theta}(x^{(t)})_k),\n",
    "$$\n",
    "\n",
    "we have that\n",
    "\n",
    "$$\n",
    "J(\\Theta) = \\frac{1}{m} \\left[\\sum_{t=1}^{m} \\sum_{k=1}^{K} c_k^{(t)}\\right] + \\frac{\\lambda}{2m} \\sum_{l=1}^{L-1} \\sum_{i=1}^{s_{l+1}} \\sum_{j=1}^{s_l} \\left(\\Theta_{ij}^{(l)}\\right)^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Computing the Gradient\n",
    "\n",
    "Before starting our analysis, let's note that for the sigmoid function\n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}},$$\n",
    "\n",
    "its derivative is\n",
    "\n",
    "$$\n",
    "g'(z) = \\frac{e^{-z}}{(1+e^{-z})^2} = \\frac{1+e^{-z} - 1}{(1+e^{-z})^2} = g(z) - g(z)^2 = g(z) (1 - g(z)).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First step: For $l=L-1$\n",
    "\n",
    "Let $i\\in\\{1, \\dots, s_L=K\\}$ and $j\\in\\{0, \\dots, s_{L-1}\\}$. Then,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\Theta_{ij}^{(L-1)}} J(\\Theta) = \\frac{1}{m} \\left[\\sum_{t=1}^{m} \\sum_{k=1}^{K} \\frac{\\partial}{\\partial \\Theta_{ij}^{(L-1)}}c_k^{(t)}\\right] + \\frac{\\lambda}{m} \\Theta_{ij}^{(L-1)} \\mathcal{I}\\{j>0\\},\n",
    "$$\n",
    "\n",
    "where $\\mathcal{I}(\\cdot)$ stands for the indicator function.\n",
    "\n",
    "![L-1](figures/L-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, \n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\Theta_{ij}^{(L-1)}}c_k^{(t)} = 0,\n",
    "$$\n",
    "\n",
    "whenever $k \\neq i$. On the other hand, using the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\Theta_{ij}^{(L-1)}}c_i^{(t)} = \\frac{\\partial}{\\partial z_{i}^{(t,L)}}c_i^{(t)} \\frac{\\partial}{\\partial \\Theta_{ij}^{(L-1)}} z_{i}^{(t,L)},\n",
    "$$\n",
    "\n",
    "where ($h_{\\Theta}(x^{(t)})_k=a_k^{(t,L)}=g(z_k^{(t,L)})$)\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial z_{i}^{(t,L)}}c_i^{(t)} & = y^{(t)}_i \\frac{1}{a^{(t,L)}_i} a^{(t,L)}_i (1 - a^{(t,L)}_i) + (1-y^{(t)}_i) \\frac{1}{1 - a^{(t,L)}_i} (-a^{(t,L)}_i (1 - a^{(t,L)}_i)) \\\\\n",
    "& = y^{(t)}_i (1 - a^{(t,L)}_i) - (1-y^{(t)}_i) a^{(t,L)}_i \\\\\n",
    "& = a^{(t,L)}_i - y_i^{(t)} \\\\\n",
    "& =: \\delta_i^{(t,L)},\n",
    "\\end{align}\n",
    "\n",
    "and $\\frac{\\partial}{\\partial \\Theta_{ij}^{(L-1)}} z_{i}^{(t,L)} = a_j^{(t,L-1)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\Theta_{ij}^{(L-1)}} J(\\Theta) = \\frac{1}{m} \\left[\\sum_{t=1}^{m} \\delta_i^{(t,L)} a_j^{(t,L-1)}\\right] + \\frac{\\lambda}{m} \\Theta_{ij}^{(L-1)} \\mathcal{I}\\{j>0\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second step: For $l=L-2$\n",
    "\n",
    "Let $i\\in\\{1, \\dots, s_{L-1}\\}$ and $j\\in\\{0, \\dots, s_{L-2}\\}$. Then,\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\Theta_{ij}^{(L-2)}} J(\\Theta) = \\frac{1}{m} \\sum_{t=1}^{m} \\frac{\\partial}{\\partial \\Theta_{ij}^{(L-2)}}\\left[\\sum_{k=1}^{K} c_k^{(t)}\\right] + \\frac{\\lambda}{m} \\Theta_{ij}^{(L-2)} \\mathcal{I}\\{j>0\\}.\n",
    "$$\n",
    "\n",
    "It is worth to notice here that every unit in the output layer $L$ depends on $\\Theta_{ij}^{(L-2)}$, for every $i, j, k$:\n",
    "\n",
    "![L-2](figures/L-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, applying the chain rule\n",
    "\n",
    "$$\n",
    " \\frac{\\partial}{\\partial \\Theta_{ij}^{(L-2)}}\\left[\\sum_{k=1}^{K} c_k^{(t)}\\right] = \\frac{\\partial}{\\partial z_{i}^{(t,L-1)}}\\left[\\sum_{k=1}^{K} c_k^{(t)}\\right] \\frac{\\partial}{\\partial \\Theta_{ij}^{(L-2)}} z_{i}^{(t,L-1)}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial z_{i}^{(t,L-1)}}\\left[\\sum_{k=1}^{K} c_k^{(t)}\\right] & = \\sum_{k=1}^{K}\\frac{\\partial}{\\partial z_{k}^{(t,L)}}c_k^{(t)} \\frac{\\partial}{\\partial a_{i}^{(t,L-1)}}z_k^{(t)} \\frac{\\partial}{\\partial z_{i}^{(t,L-1)}}a_{i}^{(t,L-1)} \\\\\n",
    "& = \\left[\\sum_{k=1}^{K} \\delta_k^{(t, L)} \\Theta_{ki}^{(L-1)}\\right] a_i^{(t,L-1)} (1 - a_i^{(t,L-1)})  \\\\\n",
    "& = (\\Theta_{:i}^{(L-1)})^T \\delta^{(t, L)} a_i^{(t,L-1)} (1 - a_i^{(t,L-1)}) \\\\\n",
    "& =: \\delta_i^{(t, L-1)},\n",
    "\\end{align}\n",
    "\n",
    "where $(\\Theta_{:i}^{(L-1)})^T = \\left[\\Theta_{1i}^{(L-1)} \\quad \\Theta_{2i}^{(L-1)} \\quad \\dots \\quad \\Theta_{s_{L}i}^{(L-1)}\\right]$ is the $i$-th column of $\\Theta^{(L-1)}$ transposed, and $\\delta^{(t, L)} = \\left[\\delta_1^{(t, L)} \\quad \\delta_2^{(t, L)} \\quad \\dots \\quad \\delta_{K}^{(t, L)}\\right]$.\n",
    "\n",
    "Moreover, $\\frac{\\partial}{\\partial \\Theta_{ij}^{(L-2)}} z_{i}^{(t,L-1)} = a_j^{(t, L-2)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\Theta_{ij}^{(L-2)}} J(\\Theta) = \\frac{1}{m} \\left[\\sum_{t=1}^{m} \\delta_i^{(t, L-1)} a_j^{(t, L-2)}\\right] + \\frac{\\lambda}{m} \\Theta_{ij}^{(L-2)} \\mathcal{I}\\{j>0\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inductive step\n",
    "\n",
    "Suppose that we've calculated all the partial derivatives of the cost function w.r.t. the parameters in the layer $l+1$ as\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\Theta_{ij}^{(l+1)}} J(\\Theta) = \\frac{1}{m} \\left[\\sum_{t=1}^{m} \\delta_i^{(t, l+2)} a_j^{(t, l+1)}\\right] + \\frac{\\lambda}{m} \\Theta_{ij}^{(l+1)} \\mathcal{I}\\{j>0\\}.\n",
    "$$\n",
    "\n",
    "for all $i\\in\\{1, \\dots, s_{l+2}\\}$ and $j\\in\\{0, \\dots, s_{l+1}\\}$, with\n",
    "\n",
    "\\begin{align}\n",
    "\\delta_i^{(t, l+2)} &:= \\frac{\\partial}{\\partial z_{i}^{(t,L-1)}} \\left[\\sum_{k=1}^{K} c_k^{(t)}\\right] \\\\\n",
    "& = (\\Theta_{:i}^{(l+2)})^T \\delta^{(t, l+3)} a_i^{(t,l+2)} (1 - a_i^{(t,l+2)}).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the layer $l$, let $i\\in\\{1, \\dots, s_{l+1}\\}$ and $j\\in\\{0, \\dots, s_{l}\\}$. Then,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = \\frac{1}{m} \\sum_{t=1}^{m} \\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}}\\left[\\sum_{k=1}^{K} c_k^{(t)}\\right] + \\frac{\\lambda}{m} \\Theta_{ij}^{(l)} \\mathcal{I}\\{j>0\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![l](figures/l.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the chain rule, we have\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}}\\left[\\sum_{k=1}^{K} c_k^{(t)}\\right] = \\frac{\\partial}{\\partial z_{i}^{(t,l+1)}}\\left[\\sum_{k=1}^{K} c_k^{(t)}\\right] \\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} z_{i}^{(t,l+1)},\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial z_{i}^{(t,l+1)}}\\left[\\sum_{k=1}^{K} c_k^{(t)}\\right] &=\\sum_{p=1}^{s_{l+2}} \\left( \\frac{\\partial}{\\partial z_{p}^{(t,l+2)}}\\left[\\sum_{k=1}^{K} c_k^{(t)}\\right] \\frac{\\partial}{\\partial a_{i}^{(t,l+1)}} z_{p}^{(t,l+2)} \\right) \\frac{\\partial}{\\partial z_{i}^{(t,l+1)}} a_{i}^{(t,l+1)}  \\\\\n",
    "& = \\left[\\sum_{p=1}^{s_{l+2}} \\delta_p^{(t, l+2)} \\Theta_{pi}^{(l+1)}\\right] a_i^{(t,l+1)} (1 - a_i^{(t,l+1)}) \\\\\n",
    "& = (\\Theta_{:i}^{(l+1)})^T \\delta^{(t, l+2)} a_i^{(t,l+1)} (1 - a_i^{(t,l+1)}) \\\\\n",
    "& =: \\delta_i^{(t,l+1)},\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $\\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} z_{i}^{(t,l+1)} = a_j^{(t,l)}$.\n",
    "\n",
    "Thus, \n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = \\frac{1}{m} \\left[\\sum_{t=1}^{m} \\delta_i^{(t, l+1)} a_j^{(t, l)}\\right] + \\frac{\\lambda}{m} \\Theta_{ij}^{(l)} \\mathcal{I}\\{j>0\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Backpropagation algorithm\n",
    "\n",
    "Based on the analysis of the gradients above, one can come up with the following backpropagation algorithm:\n",
    "\n",
    "Given the training set $\\{(x^{(1)}, y^{(1)}), \\dots (x^{(m)}, y^{(m)})\\}$:\n",
    "\n",
    "> - Set $D^{(l)} = 0\\in\\mathbb{R}^{s_{l+1} \\times (1 + s_l)}$ for $l\\in\\{1, \\dots, L-1\\}$.\n",
    "> - For $t=1, \\dots, m$:\n",
    ">     - Set $a^{(1)} = x^{(t)}$.\n",
    ">     - Forward propagation algorithm:\n",
    ">       $$\\left\\{\n",
    "        \\begin{align}\n",
    "        z^{(l)} & = \\Theta^{(l-1)} \\left[\\begin{array}{c} 1 \\\\ a^{(l-1)} \\end{array}\\right] \\in\\mathbb{R}^{s_l} \\\\\n",
    "        a^{(l)} & = g(z^{(l})\\in\\mathbb{R}^{s_l}.\n",
    "        \\end{align}\\right.\n",
    "        $$\n",
    ">       for $l\\in\\{2, \\dots, L\\}$.\n",
    ">     - Compute $\\delta^{(L)} = a^{(L)} - y^{(t)} \\in \\mathbb{R}^{s_L}$.\n",
    ">     - Compute $\\delta^{(l)} = (\\Theta_{:,1:}^{(l)})^T \\delta^{(l+1)} \\circ a^{(l)} \\circ (1 - a^{(l)})$, for $l\\in\\{L-1, \\dots, 2\\}$.\n",
    ">     - Update $D^{(l)} := D^{(l)} + \\delta^{(l+1)} \\left[1 \\quad (a^{(l)})^T\\right]$, for $l\\in\\{1, \\dots, L-1\\}$.     \n",
    "> - Update $D^{(l)} := \\frac{1}{m} \\left(D^{(l)} + \\lambda \\left[0_{s_{l+1} \\times 1} \\quad | \\quad 1_{s_{l+1} \\times s_l}\\right] \\circ \\theta^{(l)}\\right)$, for $l\\in\\{1, \\dots, L-1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we get that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = D^{(l)}_{ij}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Esteban Jiménez Rodríguez. Based on the content of the Machine Learning course offered through coursera by Prof. Andrew Ng.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
