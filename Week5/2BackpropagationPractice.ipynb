{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Unrolling Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose that we have a neural network structure with 4 layers and, $s_1=s_2=s_3=10$ and $s_4=1$.\n",
    "\n",
    "Then our matrices $\\Theta^{(l)}$ and $D^{(l)}$ will have the following dimensions:\n",
    "\n",
    "- $\\Theta^{(1)}, D^{(1)}\\in\\mathbb{R}^{10 \\times 11}$\n",
    "- $\\Theta^{(2)}, D^{(2)}\\in\\mathbb{R}^{10 \\times 11}$\n",
    "- $\\Theta^{(3)}, D^{(3)}\\in\\mathbb{R}^{1 \\times 11}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work with these in a numerical optimizer we have to accomodate them into vectors as:\n",
    "\n",
    "$$\n",
    "\\Theta_{vec} = \\left[\n",
    "\\begin{array}{c}\n",
    "vec(\\Theta^{(1)}) \\\\\n",
    "vec(\\Theta^{(2)}) \\\\\n",
    "vec(\\Theta^{(3)})\n",
    "\\end{array}\n",
    "\\right] \\in\\mathbb{R}^{331} \\qquad \\text{and} \\qquad D_{vec} = \\left[\n",
    "\\begin{array}{c}\n",
    "vec(D^{(1)}) \\\\\n",
    "vec(D^{(2)}) \\\\\n",
    "vec(D^{(3)})\n",
    "\\end{array}\n",
    "\\right]\\in\\mathbb{R}^{331}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the $vec(A)\\in\\mathbb{R}^{mn}$ operation over a matrix $A\\in\\mathbb{R}^{m \\times n}$ consists on vertically stacking the columns of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, once the numerical optimizer converges, we can rearrange the vectors into the former matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can numerically approximate the gradient in order to check that our backpropagation algorithm is working properly. To do this, consider the unrolled version of the parameters $\\Theta_{vec} = [\\theta_1, \\dots, \\theta_n]$.\n",
    "\n",
    "Then, the $i$-th partial derivative of the cost function can be approximated by the two-sided difference:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_i} J(\\Theta_{vec}) \\approx \\frac{J(\\theta_1, \\dots, \\theta_i + \\varepsilon, \\dots, \\theta_n) - J(\\theta_1, \\dots, \\theta_i - \\varepsilon, \\dots, \\theta_n)}{2\\varepsilon}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for a sufficiently small $\\varepsilon$ the approximated derivative $\\frac{\\partial}{\\partial \\theta_i} J(\\Theta_{vec})$ should be pretty close to the $i$-th component of $D_{vec}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to select the initial value of $\\Theta_{vec}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the logistic regresion and linear regresion we always initialized the parameters as zero. In those algorithms, the initialization does not matter theoretically since the cost function is convex.\n",
    "\n",
    "On the other hand, the cost function for neural networks is **not convex** in general. Moreover, initialize the parameters as zero often leads to poor performances (non-identifiability of the parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One recommendation is to initialized each $\\Theta_{ij}^{(l)}$ to a random value in $[-\\epsilon, \\epsilon]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Esteban Jiménez Rodríguez. Based on the content of the Machine Learning course offered through coursera by Prof. Andrew Ng.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
