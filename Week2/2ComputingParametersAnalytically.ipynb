{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Parameters Analytically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now we have been using the gradient descent iterative algorithm to minimize the cost function \n",
    "$J(\\boldsymbol{\\theta})$ and find the parameters $\\boldsymbol{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, the normal equation will give us a method to find the parameters $\\boldsymbol{\\theta}$ analytically. This is, rather than needing to run the gradient descent iterations, we can solve for $\\boldsymbol{\\theta}$ in only one step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we pointed out in the last lecture, the cost function can be written as:\n",
    "\n",
    "$$\n",
    "J(\\boldsymbol{\\theta}) = \\frac{1}{2m}\\left\\lvert\\left\\lvert\\boldsymbol{X}\\boldsymbol{\\theta} - y^{(i)}\\right\\rvert\\right\\rvert^2,\n",
    "$$\n",
    "\n",
    "where \n",
    "$$\n",
    "\\boldsymbol{X} = \\left[\n",
    "\\begin{array}{ccccc}\n",
    "x_0^{(1)} & x_1^{(1)} & x_2^{(1)} & \\dots  & x_n^{(1)} \\\\\n",
    "x_0^{(2)} & x_1^{(2)} & x_2^{(2)} & \\dots  & x_n^{(2)} \\\\\n",
    "\\vdots    & \\vdots    & \\vdots    & \\ddots & \\vdots    \\\\\n",
    "x_0^{(m)} & x_1^{(m)} & x_2^{(m)} & \\dots  & x_n^{(m)}\n",
    "\\end{array}\n",
    "\\right] \\in \\mathbb{R}^{m \\times (n+1)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can note a pair of things:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) = \\frac{1}{m} \\boldsymbol{X}^T (\\boldsymbol{X}\\boldsymbol{\\theta} - \\boldsymbol{y}) \\in \\mathbb{R}^{n+1},\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2}{\\partial \\boldsymbol{\\theta}^2} J(\\boldsymbol{\\theta}) = \\frac{1}{m} \\boldsymbol{X}^T \\boldsymbol{X} \\in \\mathbb{R}^{(n+1) \\times (n+1)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the one hand, the Hessian $\\frac{\\partial^2}{\\partial \\boldsymbol{\\theta}^2} J(\\boldsymbol{\\theta})$ is positive semi-definite, which assures the convexity of the cost function.\n",
    "\n",
    "This implies that any local minimum of the cost function is also a global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) = 0 \\Leftrightarrow \\frac{1}{m} \\boldsymbol{X}^T (\\boldsymbol{X}\\boldsymbol{\\theta} - \\boldsymbol{y}) = 0 \\Leftrightarrow \\boldsymbol{\\theta} = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T \\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which implies that $\\boldsymbol{\\theta} = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T \\boldsymbol{y}$ is a global minimizer of the cost function $J(\\boldsymbol{\\theta})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which method should I use?\n",
    "\n",
    "| **Gradient Descent**              | **Normal Equation**        |\n",
    "| --------------------------------- | -------------------------- |\n",
    "| Neet to choose $\\alpha$           | No need to choose $\\alpha$ |\n",
    "| Needs many iterations             | Don't need to iterate      |\n",
    "| Works well even when $n$ is large | Neet to compute $(X^T X)^{-1}$, which is slow if $n$ is large |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, the Gradient Descent method can be used to optimize generic functions, whereas the Normal equation is only for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Normal Equation Noninvertibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if $\\boldsymbol{X}^T\\boldsymbol{X}$ is not invertible (is singular)?\n",
    "\n",
    "This happens when:\n",
    "\n",
    "1. There are linearly dependent features.\n",
    "\n",
    "2. There are too many features (more than training examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any case, these problems can be avoided via a good **feature selection** before applying the linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Esteban Jiménez Rodríguez. Based on the content of the Machine Learning course offered through coursera by Prof. Andrew Ng.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
