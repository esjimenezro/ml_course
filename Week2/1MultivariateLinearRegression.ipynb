{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multiple Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we turn our attention to a new version of linear regression which is more powerful due to its ability to work with multiple variables/features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, in our previous example of Portland house pricing we had:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "data = pd.read_csv(\"house_pricing.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2104</td>\n",
       "      <td>399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1600</td>\n",
       "      <td>329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400</td>\n",
       "      <td>369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1416</td>\n",
       "      <td>232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000</td>\n",
       "      <td>539900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   size   price\n",
       "0  2104  399900\n",
       "1  1600  329900\n",
       "2  2400  369000\n",
       "3  1416  232000\n",
       "4  3000  539900"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show data\n",
    "data[['size', 'price']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... a single feature $x$, namely the size of the houses in $ft^2$, and we wanted to use it to predict $y$, the price of the house in USD. \n",
    "\n",
    "With this, the form of our hypothesis was:\n",
    "\n",
    "$$h_{\\theta}(x) = \\theta_0 + \\theta_1 x.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is easy to imagine that we won't only have the size of the house available, but also the number of bedrooms, the number of floors, the number of car spaces, the number of bathrooms, the age of the house, among others, and our data would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>n_bedrooms</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2104</td>\n",
       "      <td>3</td>\n",
       "      <td>399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1600</td>\n",
       "      <td>3</td>\n",
       "      <td>329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400</td>\n",
       "      <td>3</td>\n",
       "      <td>369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1416</td>\n",
       "      <td>2</td>\n",
       "      <td>232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000</td>\n",
       "      <td>4</td>\n",
       "      <td>539900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   size  n_bedrooms   price\n",
       "0  2104           3  399900\n",
       "1  1600           3  329900\n",
       "2  2400           3  369000\n",
       "3  1416           2  232000\n",
       "4  3000           4  539900"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These additional features give us much more information in our target of predicting the price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some notation:\n",
    "\n",
    "- $m$: Number of training examples.\n",
    "- $n$: Number of input variables/features.\n",
    "- $\\boldsymbol{x}=\\left[x_1, \\dots, x_n\\right]^T\\in\\mathbb{R}^n$: \"Input\" variables/features.\n",
    "- $x_j, \\quad j=1,2,\\dots,n$: $j-th$ input variable/feature.\n",
    "- $y$: \"Output\" variable/\"target\" variable.\n",
    "- $\\boldsymbol{x}^{(i)}$: Input (features) of $i-$th training example.\n",
    "- $\\boldsymbol{x}^{(i)}_j$: Value of feature $j$ in $i-$th training example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, in the above data we may define $x_1$ as the house size, and $x_2$ as the number of bedrooms. In this sense:\n",
    "\n",
    "- $n=2$ is the number of input features.\n",
    "\n",
    "- The fourth training example is:\n",
    "  $$\n",
    "  x^{(4)} = \\left[\n",
    "  \\begin{array}{c}\n",
    "  1416 \\\\\n",
    "  2\n",
    "  \\end{array}\n",
    "  \\right]\\in\\mathbb{R}^{2}.\n",
    "  $$\n",
    "    \n",
    "- The value of the second feature in the fourth training example:\n",
    "  $$x^{(4)}_2 = 2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have **multiple features**, the form of our hypothesis turns into:\n",
    "\n",
    "$$\n",
    "h_{\\theta}(\\boldsymbol{x}) = \\theta_0 + \\theta_1 x_1 + \\dots + \\theta_n x_n.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we conveniently define a $0$ feature: $x_0 = 1$ ($x^{(i)}_0=1$ for all $i\\in\\{1, \\dots, m\\}$), the new feature vector $\\boldsymbol{x}$ takes the form:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}=\\left[\n",
    "\\begin{array}{c}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{array}\n",
    "\\right]\\in\\mathbb{R}^{n+1}\n",
    "$$\n",
    "\n",
    "as well as the parameter vector $\\boldsymbol{\\theta}$:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta}=\\left[\n",
    "\\begin{array}{c}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_n\n",
    "\\end{array}\n",
    "\\right]\\in\\mathbb{R}^{n+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking these into account, we can rewrite the hypothesis function as:\n",
    "\n",
    "\\begin{align}\n",
    "h_{\\theta}(\\boldsymbol{x}) & = \\theta_0 x_0 + \\theta_1 x_1 + \\dots + \\theta_n x_n \\\\\n",
    "                           & = \\boldsymbol{x}^T \\boldsymbol{\\theta} \\\\\n",
    "                           & = \\boldsymbol{\\theta}^T \\boldsymbol{x}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient Descent for Multiple Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above notation for the hypothesis function, we can define the cost function for multivariate linear regression in the same way we defined it for univariate linear regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J(\\boldsymbol{\\theta}) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}(\\boldsymbol{x}^{(i)}) - y^{(i)})^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the parameter vector $\\boldsymbol{\\theta}$ is the $n+1$ dimensional vector:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta}=\\left[\n",
    "\\begin{array}{c}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_n\n",
    "\\end{array}\n",
    "\\right]\\in\\mathbb{R}^{n+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently, considering a matrix of all the training examples $\\boldsymbol{X}$:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X} = \\left[\n",
    "\\begin{array}{c}\n",
    "\\boldsymbol{x}^{(1)} \\ ^T \\\\\n",
    "\\boldsymbol{x}^{(2)} \\ ^T \\\\\n",
    "\\vdots                    \\\\\n",
    "\\boldsymbol{x}^{(n)} \\ ^T\n",
    "\\end{array}\n",
    "\\right] = \\left[\n",
    "\\begin{array}{ccccc}\n",
    "x_0^{(1)} & x_1^{(1)} & x_2^{(1)} & \\dots  & x_n^{(1)} \\\\\n",
    "x_0^{(2)} & x_1^{(2)} & x_2^{(2)} & \\dots  & x_n^{(2)} \\\\\n",
    "\\vdots    & \\vdots    & \\vdots    & \\ddots & \\vdots    \\\\\n",
    "x_0^{(m)} & x_1^{(m)} & x_2^{(m)} & \\dots  & x_n^{(m)}\n",
    "\\end{array}\n",
    "\\right] = \\left[\n",
    "\\begin{array}{ccccc}\n",
    "1         & x_1^{(1)} & x_2^{(1)} & \\dots  & x_n^{(1)} \\\\\n",
    "1         & x_1^{(2)} & x_2^{(2)} & \\dots  & x_n^{(2)} \\\\\n",
    "\\vdots    & \\vdots    & \\vdots    & \\ddots & \\vdots    \\\\\n",
    "1         & x_1^{(m)} & x_2^{(m)} & \\dots  & x_n^{(m)}\n",
    "\\end{array}\n",
    "\\right] \\in \\mathbb{R}^{m \\times (n+1)}\n",
    "$$\n",
    "\n",
    "The cost function can be rewritten as:\n",
    "\n",
    "$$\n",
    "J(\\boldsymbol{\\theta}) = \\frac{1}{2m}\\sum_{i=1}^{m}(\\boldsymbol{x}^{(i)} \\ ^T \\boldsymbol{\\theta} - y^{(i)})^2 = \\frac{1}{2m}\\left\\lvert\\left\\lvert\\boldsymbol{X}\\boldsymbol{\\theta} - \\boldsymbol{y}\\right\\rvert\\right\\rvert^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, the gradient descent algorithm takes the same form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize $\\theta_j$, for $j\\in\\{0, 1,\\dots, n\\}$.\n",
    "\n",
    "- repeat until convergence {\n",
    "  $$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\boldsymbol{\\theta}); \\qquad \\text{ for } j\\in\\{0, 1,\\dots, n\\}$$\n",
    "  }\n",
    "  \n",
    "where:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\theta_j} J(\\boldsymbol{\\theta}) & = \\frac{1}{m}\\sum_{i=1}^{m}\\left(h_{\\theta}(\\boldsymbol{x}^{(i)})-y^{(i)}\\right)x_j^{(i)} \\qquad \\text{ for } j\\in\\{0, 1,\\dots, n\\}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, in a vector form:\n",
    "\n",
    "- Initialize $\\boldsymbol{\\theta}$.\n",
    "\n",
    "- repeat until convergence {\n",
    "  $$\\boldsymbol{\\theta} := \\boldsymbol{\\theta} - \\alpha \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} J(\\boldsymbol{\\theta})$$\n",
    "  }\n",
    "  \n",
    "where $\\frac{\\partial}{\\partial \\boldsymbol{\\theta}} J(\\boldsymbol{\\theta})$ is the gradient of the function $J$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) = \\frac{1}{m}\\left[\n",
    "\\begin{array}{c}\n",
    "\\sum_{i=1}^{m}\\left(h_{\\theta}(\\boldsymbol{x}^{(i)})-y^{(i)}\\right)x_0^{(i)} \\\\\n",
    "\\sum_{i=1}^{m}\\left(h_{\\theta}(\\boldsymbol{x}^{(i)})-y^{(i)}\\right)x_1^{(i)} \\\\\n",
    "\\vdots \\\\\n",
    "\\sum_{i=1}^{m}\\left(h_{\\theta}(\\boldsymbol{x}^{(i)})-y^{(i)}\\right)x_n^{(i)}\n",
    "\\end{array}\n",
    "\\right] = \\frac{1}{m} \\boldsymbol{X}^T (\\boldsymbol{X}\\boldsymbol{\\theta} - \\boldsymbol{y})\\in\\mathbb{R}^{n+1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Feature scaling\n",
    "\n",
    "One problem that appears when we have multiple features, is that each one of the is in a different scale probably.\n",
    "\n",
    "For instance in our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>n_bedrooms</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2104</td>\n",
       "      <td>3</td>\n",
       "      <td>399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1600</td>\n",
       "      <td>3</td>\n",
       "      <td>329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400</td>\n",
       "      <td>3</td>\n",
       "      <td>369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1416</td>\n",
       "      <td>2</td>\n",
       "      <td>232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000</td>\n",
       "      <td>4</td>\n",
       "      <td>539900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   size  n_bedrooms   price\n",
       "0  2104           3  399900\n",
       "1  1600           3  329900\n",
       "2  2400           3  369000\n",
       "3  1416           2  232000\n",
       "4  3000           4  539900"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(852, 4478)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_1 feature range\n",
    "data['size'].min(), data['size'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_1 feature range\n",
    "data['n_bedrooms'].min(), data['n_bedrooms'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The $x_1$ feature (size) varies in the range $0 - 4500$ $ft^2$.\n",
    "- The $x_2$ feature (number of bedrooms) varies in the range $1 - 5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good idea is to **scale** these features so that they vary within similar ranges. Following this idea, the gradient descent algorithm can converge more quikcly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concretely, we may select instead of the above features:\n",
    "\n",
    "- $x_1 = \\frac{\\text{size }(ft^2)}{5000}$\n",
    "- $x_2 = \\frac{\\text{number of bedrooms}}{5}$\n",
    "\n",
    "so that $0 \\leq x_1 \\leq 1$ and $0 \\leq x_2 \\leq 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a feature scaling approach consists on getting every feature into approximately a $-1 \\leq x_i \\leq 1$ range ($i\\in\\{1, \\dots, n\\}$; the feature $x_0=1$ is excluded)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to do this is:\n",
    "\n",
    "1. **Mean normalization:** Replace each feature $x_i$ with $x_i - \\mu_i$ to make features have approximately zero mean (do not apply to $x_0=1$).\n",
    "\n",
    "2. **Scale:** After the mean normalization, the next step is to scale the resulting feature according to some factor (the span of the variable $\\max x_i-\\min x_i$ or the standard deviation $\\sigma_i$. This is\n",
    "   \n",
    "   $$\n",
    "   x_i \\to \\frac{x_i - \\mu_i}{s_i},\n",
    "   $$\n",
    "   \n",
    "   where $s_i$ may be one of $\\max x_i-\\min x_i$ and $\\sigma_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Learning rate\n",
    "\n",
    "The gradient descent algorithm has the form\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta} := \\boldsymbol{\\theta} - \\alpha \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}).\n",
    "$$\n",
    "\n",
    "One natural question that arises is: How to choose the **learning rate** $\\alpha$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing is to make sure that the **gradient descent algorithm** is working properly. This can be done by looking at the cost function vs. the number of iterations plot. It should look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '$J(\\\\theta)$')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Decreasing cost function\n",
    "plt.figure(figsize=(6, 4))\n",
    "x = np.linspace(0, 10)\n",
    "plt.plot(x, 11 * np.exp(-x / 3), label='')\n",
    "plt.axvline(x=0, c='k', lw=2)\n",
    "plt.axhline(y=0, c='k', lw=2)\n",
    "plt.tick_params(\n",
    "    axis='both',\n",
    "    which='both',\n",
    "    bottom=False,\n",
    "    top=False,\n",
    "    left=False,\n",
    "    right=False,\n",
    "    labelbottom=False,\n",
    "    labeltop=False,\n",
    "    labelleft=False,\n",
    "    labelright=False)\n",
    "plt.axis([-1, 10, -1, 10])\n",
    "plt.xlabel('No. of iterations')\n",
    "plt.ylabel(r'$J(\\theta)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the gradient descent algorithm:\n",
    "\n",
    "- The cost function $J(\\boldsymbol{\\theta})$ should decrease after every iteration.\n",
    "- The number of iterations it takes to converge varies significantly across different applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the cost function increases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '$J(\\\\theta)$')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Increasing cost function\n",
    "plt.figure(figsize=(6, 4))\n",
    "x = np.linspace(0, 10)\n",
    "plt.plot(x, 4 * np.exp(x / 10), label='')\n",
    "plt.axvline(x=0, c='k', lw=2)\n",
    "plt.axhline(y=0, c='k', lw=2)\n",
    "plt.tick_params(\n",
    "    axis='both',\n",
    "    which='both',\n",
    "    bottom=False,\n",
    "    top=False,\n",
    "    left=False,\n",
    "    right=False,\n",
    "    labelbottom=False,\n",
    "    labeltop=False,\n",
    "    labelleft=False,\n",
    "    labelright=False)\n",
    "plt.axis([-1, 10, -1, 10])\n",
    "plt.xlabel('No. of iterations')\n",
    "plt.ylabel(r'$J(\\theta)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then it is a clear signal that the gradient descent algorithm is not working. The reason is that the selected learning rate $\\alpha$ is too big.\n",
    "\n",
    "> For **sufficently small** $\\alpha$, $J(\\theta)$ should decrease on every iteration.\n",
    "\n",
    "> But if $\\alpha$ is too small, gradient descent can be slow to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Features and Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not mandatory to use the data \"as is\". We can perform some operations on the data to obtain new features that may make more sense to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, consider the case where we are given the frontage and the depth of a house in order to predict its price. As before, we can use a hypothesis function like\n",
    "\n",
    "$$\n",
    "h_{\\theta}(\\boldsymbol{x}) = \\theta_0 + \\theta_1 \\times frontage + \\theta_2 \\times depth.\n",
    "$$\n",
    "\n",
    "On the other hand, we may think that what actually determines the price of the house is its size. Then, we can define a feature $x = frontage \\times depth$ and use the hypothesis function\n",
    "\n",
    "$$\n",
    "h_{\\theta}(x) = \\theta_0+ \\theta_1 x.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use all the machinery we have developed for multivariate linear regression to fit polynomial hypotheses to our data.\n",
    "\n",
    "For instance, we could try a cubic hypothesis like:\n",
    "\n",
    "$$\n",
    "h_{\\theta}(x) = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\theta_3 x^3.\n",
    "$$\n",
    "\n",
    "We can think of it as a multivariate hypothesis:\n",
    "\n",
    "\\begin{align}\n",
    "h_{\\theta}(x) & = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 \\\\\n",
    "              & = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\theta_3 x^3\n",
    "\\end{align}\n",
    "\n",
    "with $x_1 = x$, $x_2 = x^2$ and $x_3 = x^3$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we choose the features like this, then the feature scaling becomes very important. For example, if $x$ is the house size, then we have that\n",
    "\n",
    "$$\n",
    "x_1 \\sim 1000\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_2 \\sim 1000000\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_3 \\sim 1000000000\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Esteban Jiménez Rodríguez. Based on the content of the Machine Learning course offered through coursera by Prof. Andrew Ng.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
