{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cost function\n",
    "\n",
    "We assume that we have a training set $\\{(\\boldsymbol{x}^{(1)}, y^{(1)}), (\\boldsymbol{x}^{(2)}, y^{(2)}), \\dots, (\\boldsymbol{x}^{(m)}, y^{(m)})\\}$ with $m$ examples, where \n",
    "\n",
    "$$x=\\left[\n",
    "\\begin{array}{c}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "with $x_0 = 1$ and $y\\in\\{0, 1\\}.$\n",
    "\n",
    "The hypothesis function is:\n",
    "\n",
    "$$\n",
    "h_{\\theta}(\\boldsymbol{x}) = \\frac{1}{1 + e^{-\\boldsymbol{\\theta}^T\\boldsymbol{x}}} = \\frac{1}{1 + e^{-\\boldsymbol{x}^T\\boldsymbol{\\theta}}}\n",
    "$$\n",
    "\n",
    "How can we choose the parameters $\\theta$? Does the linear regression cost works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the linear regression cost could be expressed as:\n",
    "\n",
    "$$\n",
    "J(\\boldsymbol{\\theta}) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}(\\boldsymbol{x}^{(i)}) - y^{(i)})^2.\n",
    "$$\n",
    "\n",
    "It turns out that for this hypothesis, this cost function is not convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's consider the **logistic regression cost function**:\n",
    "\n",
    "$$\n",
    "J(\\boldsymbol{\\theta}) = \\frac{1}{m}\\sum_{i=1}^{m} \\mathrm{cost}(h_{\\theta}(\\boldsymbol{x}^{(i)}), y^{(i)}),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathrm{cost}(h_{\\theta}(\\boldsymbol{x}), y) = \\left\\lbrace\n",
    "\\begin{array}{ccc}\n",
    "-\\log(h_{\\theta}(\\boldsymbol{x})) & \\mathrm{if} & y=1 \\\\\n",
    "-\\log(1-h_{\\theta}(\\boldsymbol{x})) & \\mathrm{if} & y=0.\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $\\mathrm{cost}(h_{\\theta}(\\boldsymbol{x}), y) = 0$:\n",
    "- If $y=1$ and $h_{\\theta}(\\boldsymbol{x})=1$, or\n",
    "- If $y=0$ and $h_{\\theta}(\\boldsymbol{x})=0$.\n",
    "\n",
    "Otherwise, if for example $h_{\\theta}(\\boldsymbol{x})\\to 0$ and $y=1$, then $\\mathrm{cost}(h_{\\theta}(\\boldsymbol{x}), y)\\to \\infty$.\n",
    "\n",
    "Then, this cost function captures the desired behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simplified Cost Function and Gradient Descent\n",
    "\n",
    "Note that the term $\\mathrm{cost}(h_{\\theta}(\\boldsymbol{x}), y)$ can be written in only one expression as:\n",
    "\n",
    "$$\n",
    "\\mathrm{cost}(h_{\\theta}(\\boldsymbol{x}), y) = \\left\\lbrace\n",
    "\\begin{array}{ccc}\n",
    "-\\log(h_{\\theta}(\\boldsymbol{x})) & \\mathrm{if} & y=1 \\\\\n",
    "-\\log(1-h_{\\theta}(\\boldsymbol{x})) & \\mathrm{if} & y=0.\n",
    "\\end{array}\n",
    "\\right. = -y\\log(h_{\\theta}(\\boldsymbol{x})) - (1-y)\\log(1-h_{\\theta}(\\boldsymbol{x})).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the logistic regression cost function can be rewritten as:\n",
    "\n",
    "$$\n",
    "J(\\boldsymbol{\\theta}) = -\\frac{1}{m}\\sum_{i=1}^{m} \\left[y^{(i)}\\log(h_{\\theta}(\\boldsymbol{x}^{(i)})) + (1-y^{(i)})\\log(1-h_{\\theta}(\\boldsymbol{x}^{(i)}))\\right],\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, recalling that \n",
    "\n",
    "$$\n",
    "h_{\\theta}(\\boldsymbol{x}) = \\frac{1}{1 + e^{-\\boldsymbol{\\theta}^T\\boldsymbol{x}}} = \\frac{1}{1 + e^{-\\boldsymbol{x}^T\\boldsymbol{\\theta}}},\n",
    "$$\n",
    "\n",
    "we can write this cost function in a vectorized form as:\n",
    "\n",
    "$$\n",
    "J(\\boldsymbol{\\theta}) = -\\frac{1}{m} \\left[y^T \\log\\left(\\frac{1}{1 + e^{-\\boldsymbol{X}\\boldsymbol{\\theta}}}\\right) + (1- y)^T \\log\\left(\\frac{e^{-\\boldsymbol{X}\\boldsymbol{\\theta}}}{1 + e^{-\\boldsymbol{X}\\boldsymbol{\\theta}}}\\right)\\right],\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X} = \\left[\n",
    "\\begin{array}{c}\n",
    "\\boldsymbol{x}^{(1)} \\ ^T \\\\\n",
    "\\boldsymbol{x}^{(2)} \\ ^T \\\\\n",
    "\\vdots                    \\\\\n",
    "\\boldsymbol{x}^{(n)} \\ ^T\n",
    "\\end{array}\n",
    "\\right] = \\left[\n",
    "\\begin{array}{ccccc}\n",
    "x_0^{(1)} & x_1^{(1)} & x_2^{(1)} & \\dots  & x_n^{(1)} \\\\\n",
    "x_0^{(2)} & x_1^{(2)} & x_2^{(2)} & \\dots  & x_n^{(2)} \\\\\n",
    "\\vdots    & \\vdots    & \\vdots    & \\ddots & \\vdots    \\\\\n",
    "x_0^{(m)} & x_1^{(m)} & x_2^{(m)} & \\dots  & x_n^{(m)}\n",
    "\\end{array}\n",
    "\\right] = \\left[\n",
    "\\begin{array}{ccccc}\n",
    "1         & x_1^{(1)} & x_2^{(1)} & \\dots  & x_n^{(1)} \\\\\n",
    "1         & x_1^{(2)} & x_2^{(2)} & \\dots  & x_n^{(2)} \\\\\n",
    "\\vdots    & \\vdots    & \\vdots    & \\ddots & \\vdots    \\\\\n",
    "1         & x_1^{(m)} & x_2^{(m)} & \\dots  & x_n^{(m)}\n",
    "\\end{array}\n",
    "\\right] \\in \\mathbb{R}^{m \\times (n+1)}\n",
    "$$\n",
    "\n",
    "is the matrix of all the training examples, and the functions $e^{(\\cdot)}$ and $\\log{(\\cdot)}$ are understood as the componentwise application of the exponential and logarithm functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above vectorization, the gradient of the cost function is:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) ^T &= - \\frac{1}{m} \\left[y^T \\left(1 + e^{-\\boldsymbol{X}\\boldsymbol{\\theta}}\\right) - (1- y)^T \\left(\\frac{1 + e^{-\\boldsymbol{X}\\boldsymbol{\\theta}}}{e^{-\\boldsymbol{X}\\boldsymbol{\\theta}}}\\right)\\right] \\frac{\\partial}{\\partial \\boldsymbol{\\theta}}\\left(\\frac{1}{1 + e^{-\\boldsymbol{X}\\boldsymbol{\\theta}}}\\right)^T \\\\\n",
    "&= - \\frac{1}{m} \\left[y^T \\left(1 + e^{-\\boldsymbol{X}\\boldsymbol{\\theta}}\\right) - (1- y)^T \\left(\\frac{1 + e^{-\\boldsymbol{X}\\boldsymbol{\\theta}}}{e^{-\\boldsymbol{X}\\boldsymbol{\\theta}}}\\right)\\right] \\left(\\frac{e^{-\\boldsymbol{X}\\boldsymbol{\\theta}}}{\\left(1 + e^{-\\boldsymbol{X}\\boldsymbol{\\theta}}\\right)^2}\\right)^T \\boldsymbol{X}\\\\\n",
    "&= - \\frac{1}{m} \\left[y^T \\left(1 - \\frac{1}{1 + e^{-\\boldsymbol{X}\\boldsymbol{\\theta}}}\\right) - (1- y)^T \\left(\\frac{1}{1 + e^{-\\boldsymbol{X}\\boldsymbol{\\theta}}}\\right)\\right] \\boldsymbol{X}\\\\\n",
    "&= \\frac{1}{m} \\left[\\left(\\frac{1}{1 + e^{-\\boldsymbol{X}\\boldsymbol{\\theta}}}\\right)^T - y^T\\right] \\boldsymbol{X}\n",
    "\\end{align}\n",
    "\n",
    "This is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) = \\frac{1}{m} \\boldsymbol{X}^T \\left[\\left(\\frac{1}{1 + e^{-\\boldsymbol{X}\\boldsymbol{\\theta}}}\\right) - y\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the gradient, we can apply some numerical optimization method to minimize $J(\\boldsymbol{\\theta})$ and find the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Esteban Jiménez Rodríguez. Based on the content of the Machine Learning course offered through coursera by Prof. Andrew Ng.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
